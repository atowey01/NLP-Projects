{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118d7afb",
   "metadata": {},
   "source": [
    "- **Author:** Aisling Towey\n",
    "- **Date:** 25th August 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb3939",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70fede",
   "metadata": {},
   "source": [
    "The dataset used in this analysis contains 50k IMDB movie reviews labelled as either positive or negative. It can be downloaded at https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews.\n",
    "    \n",
    "This aim of this analysis is to create a supervised text classification model that predicts with confidence whether a movie review is positive or negative. This is a very clean and balanced dataset meaning the focus here will not be on preprocessing but rather building and training the model.\n",
    "\n",
    "Transfer learning is used for this sentiment model meaning a bert-base-uncased model is loaded in from the Huggingface transformers library and used as a starting point for the model. Bert was pretrained on unlabelled text to understand the general facets of language and can be fine tuned for other tasks such as text classification. Bert models have helped achieve state of the art results in recent years due to its bidirectional training meaning it considers context from both left and right of each token. Bert also uses the transformer architecture and attention mechanism to focus on tokens with more importance. The keras library is used to fine tune the model in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0686d8e",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151a1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "# !pip install pandas\n",
    "# !pip install tensorflow\n",
    "# !pip install transformers\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f457270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\n",
    "from tensorflow.keras.models import model_from_json, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger \n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast, AutoTokenizer, AutoConfig, TFAutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoConfig\n",
    "import arrow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from pprint import pprint as pp\n",
    "\n",
    "runtime_id = arrow.utcnow().isoformat()[0:10]\n",
    "logname = 'training_' + arrow.utcnow().isoformat()[0:16]\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f0d1a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f2a7d",
   "metadata": {},
   "source": [
    "First get a general overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5ccdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "\n",
      "Class split: \n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('imdb_data.csv')\n",
    "print(data.info())\n",
    "print(f'\\nClass split: \\n{data[\"sentiment\"].value_counts()}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a21f5",
   "metadata": {},
   "source": [
    "As there is a lot of rows in this dataset we are going to take a sample of the data to speed up training time. We will then split the data into a training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42669460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training data:  5760\n",
      "Number of rows in testing data:  1600\n",
      "Number of rows in validation data:  640\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data before spliting into train, val and test sets incase it was ordered in some way\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data.sample(8000)\n",
    "# set up training, val and test sets\n",
    "train_df, test_df = train_test_split(data, test_size = 0.2, stratify = data.sentiment, random_state = 12)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 0.1, stratify = train_df.sentiment, random_state =12)\n",
    "\n",
    "# train_df.to_csv(\"train_df.csv\", index = False)\n",
    "# val_df.to_csv(\"val_df.csv\", index = False)\n",
    "# test_df.to_csv(\"test_df.csv\", index = False)\n",
    "\n",
    "print(f\"Number of rows in training data: \", len(train_df))\n",
    "print(f\"Number of rows in testing data: \", len(test_df))\n",
    "print(f\"Number of rows in validation data: \", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0343708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2902\n",
       "negative    2858\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15e7f1",
   "metadata": {},
   "source": [
    "Now we can load the tokenizer and tokenize the text in the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978e02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LENGTH = 120 # train_df['customer_query'].str.split().str.len().mean() - to check average length\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.output_hidden_states = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = MODEL_NAME, config = config)\n",
    "transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config = config)\n",
    "transformer_model = transformer_model.layers[0]\n",
    "        \n",
    "encoder = OneHotEncoder()\n",
    "train_df['label'] = pd.Categorical(train_df['sentiment'])\n",
    "val_df['label'] = pd.Categorical(val_df['sentiment'])\n",
    "\n",
    "# one hot encode the labels\n",
    "train_df['one_hot'] = encoder.fit_transform(np.array(train_df['label']).reshape(-1, 1)).toarray().tolist()\n",
    "val_df['one_hot'] = encoder.fit_transform(np.array(val_df['label']).reshape(-1, 1)).toarray().tolist()\n",
    "        \n",
    "# Prepare labels for the model\n",
    "y_train = tf.convert_to_tensor(train_df['one_hot'].tolist())\n",
    "y_val = tf.convert_to_tensor(val_df['one_hot'].tolist())\n",
    "\n",
    "# Tokenize the training set queries\n",
    "x_train = tokenizer(\n",
    "    text=train_df['review'].to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "# Tokenize the validation set queries\n",
    "x_val = tokenizer(\n",
    "    text=val_df['review'].to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "topic_list = list(encoder.categories_[0])\n",
    "topic_dict = {v:k for v,k in enumerate(topic_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c4995",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0beb22a",
   "metadata": {},
   "source": [
    "Now we can prepare the training architecture which includes loading in the bert model and adding two dropout and two dense layers, the final one with two units to equal the number of classes we are trying to classify into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee5c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_BERT_LAYER = True\n",
    "KERNEL_INITIALIZER = \"random_normal\"\n",
    "NUMBER_OF_CLASSES = 2 # exclude other from this count\n",
    "LEARNING_RATE = 5e-5\n",
    "OPTIMIZER = Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS = BinaryCrossentropy(from_logits = True)\n",
    "METRIC = BinaryAccuracy('accuracy')\n",
    "         \n",
    "# callbacks\n",
    "SAVE_BEST_MODEL = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', save_weights_only=False)\n",
    "STOP_EARLY = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "CSV_LOGGER = CSVLogger('keras_multilabel_log_{}.log'.format(runtime_id), append=False)\n",
    "\n",
    "# predictions\n",
    "PROBABILITY_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4c2f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          ((None, 120, 768), ( 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            202         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,559,342\n",
      "Trainable params: 109,559,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model input\n",
    "input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='attention_mask', dtype='int32') \n",
    "inputs = [input_ids, attention_mask]\n",
    "\n",
    "# load the Transformers BERT model as a layer in a Keras model\n",
    "bert_model = transformer_model(inputs)[1]\n",
    "dropout = Dropout(0.1)\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "# build the model output\n",
    "pooled_output = Dense(units=100)(pooled_output)\n",
    "pooled_output = tf.keras.layers.Dropout(0.2)(pooled_output)\n",
    "model_output = Dense(units=NUMBER_OF_CLASSES, kernel_initializer=KERNEL_INITIALIZER)(pooled_output)\n",
    "\n",
    "# combine it all in a model object\n",
    "model = Model(inputs=inputs, outputs=model_output)\n",
    "\n",
    "# we can only train the layers after the bert layer if we want but it seems to work better training all\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe9043",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782b9fb",
   "metadata": {},
   "source": [
    "Time to train the model! We have added callbacks here to save the model after every epoch if the validation loss is lower than the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d33e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.7834WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_test_batch_end` time: 0.4461s). Check your callbacks.\n",
      "180/180 [==============================] - 254s 1s/step - loss: 0.4291 - accuracy: 0.7834 - val_loss: 0.3480 - val_accuracy: 0.8305\n",
      "Epoch 2/5\n",
      "180/180 [==============================] - 254s 1s/step - loss: 0.2281 - accuracy: 0.9134 - val_loss: 0.4047 - val_accuracy: 0.8164\n",
      "Epoch 3/5\n",
      "180/180 [==============================] - 254s 1s/step - loss: 0.1175 - accuracy: 0.9590 - val_loss: 0.3828 - val_accuracy: 0.8508\n",
      "Epoch 4/5\n",
      "180/180 [==============================] - 254s 1s/step - loss: 0.0681 - accuracy: 0.9774 - val_loss: 0.5734 - val_accuracy: 0.8273\n",
      "Epoch 5/5\n",
      "180/180 [==============================] - 254s 1s/step - loss: 0.0423 - accuracy: 0.9865 - val_loss: 0.5116 - val_accuracy: 0.8586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c08478470>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = OPTIMIZER,\n",
    "    loss = LOSS, \n",
    "    metrics = METRIC)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "     x=[x_train['input_ids'], x_train['attention_mask']],\n",
    "    y=y_train,\n",
    "    validation_data=([x_val['input_ids'], x_val['attention_mask']], y_val),\n",
    "    callbacks=[STOP_EARLY, CSV_LOGGER],\n",
    "    callbacks=[SAVE_BEST_MODEL, STOP_EARLY, CSV_LOGGER],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS)\n",
    "\n",
    "# # Save the final model\n",
    "# model.save('training_outputs/models/final_model.h5', include_optimizer=False) \n",
    "# model.load_weights('training_outputs/models/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362486e",
   "metadata": {},
   "source": [
    "# Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1d84b",
   "metadata": {},
   "source": [
    "Now we can use the model to get some performance metrics on the unseen test set.\n",
    "\n",
    "If you have a multilabel problem where there are multiple topics to be trained and one query can fall under multiple topics a sigmoid activation is used to ensure each topic has a prediction probability between 0 and 1 rather than all topic prediction probabilities adding to 1. If you want the prediction probabilites to sum to 1 you can use a softmax activation function. We will go with softmax here as the reviews have been labelled as either positive or negative although in theory a review could be both positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f6d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Test set accuracy: 0.8475'\n",
      "array([[680, 114],\n",
      "       [130, 676]])\n",
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '    negative      0.840     0.856     0.848       794\\n'\n",
      " '    positive      0.856     0.839     0.847       806\\n'\n",
      " '\\n'\n",
      " '    accuracy                          0.848      1600\\n'\n",
      " '   macro avg      0.848     0.848     0.847      1600\\n'\n",
      " 'weighted avg      0.848     0.848     0.847      1600\\n')\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "model_name =  'bert-base-uncased'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)        \n",
    " \n",
    "test_x = tokenizer(\n",
    "text=test_df['review'].to_list(),\n",
    "add_special_tokens=True,\n",
    "max_length=MAX_LENGTH,\n",
    "truncation=True,\n",
    "padding=True, \n",
    "return_tensors='tf',\n",
    "return_token_type_ids = False,\n",
    "return_attention_mask = True,\n",
    "verbose = True)\n",
    " \n",
    "y_pred = model.predict([test_x['input_ids'], test_x['attention_mask']])\n",
    "softmax_pred = tf.nn.softmax(y_pred)\n",
    "max_prob = softmax_pred.numpy().max(axis=1)\n",
    "test_df['max_prob'] = max_prob\n",
    "softmax_pred = np.array(softmax_pred).tolist()\n",
    "test_df['softmax_pred'] = softmax_pred\n",
    "\n",
    "argmax = y_pred.argmax(axis=1)\n",
    "test_df['argmax'] = argmax\n",
    "test_df['topic_prediction']= test_df['argmax'].map(topic_dict)\n",
    "        \n",
    "targets = test_df['sentiment'].tolist()\n",
    "predictions = test_df['topic_prediction'].tolist()\n",
    "\n",
    "# output of results for csv\n",
    "results_list.append({\n",
    "                \"test_accuracy\": accuracy_score(targets, predictions),\n",
    "                \"precision_macro\": round(precision_score(targets, predictions, average='macro'), 3),\n",
    "                \"recall_macro\": round(recall_score(targets, predictions, average='macro'), 3),\n",
    "                \"f1_score_macro\": round(f1_score(targets, predictions, average='macro'), 3),\n",
    "                \"classiciation_report\": classification_report(targets, predictions, digits=3),\n",
    "                \"confusion_matrix\": confusion_matrix(targets, predictions),\n",
    "                \"max_length\": MAX_LENGTH,\n",
    "                \"num_epochs\": NUM_EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"learning_rate\": LEARNING_RATE  \n",
    "                 })\n",
    "\n",
    "# save results to csv\n",
    "results_test_df = pd.DataFrame(results_list)\n",
    "results_test_df.to_csv(\"multilabel_results_{}.csv\".format(runtime_id),\n",
    "                                 index=False, columns=[\"test_accuracy\",  \"precision_macro\", \"recall_macro\", \n",
    "                                                       \"f1_score_macro\", \"classiciation_report\", \"confusion_matrix\",\n",
    "                                                      \"max_length\", \"num_epochs\", \"batch_size\", \"learning_rate\"])\n",
    "result_dict = next(item for item in results_list)\n",
    "pp(f'Test set accuracy: {accuracy_score(targets, predictions)}')\n",
    "pp(result_dict.get('confusion_matrix'))\n",
    "pp(result_dict.get('classiciation_report'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ae74b",
   "metadata": {},
   "source": [
    "The results above, with a test set accuracy score of 85% and a f1 score around 0.83 are pretty good considering we only used a sample of the dataset and have additional data available for improved training. Other parameters can also be changed to make improvements such as the number of epochs, learning rate, batch size etc. We can look at examples in the test set the model predicted incorrectly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf57518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>softmax_pred</th>\n",
       "      <th>argmax</th>\n",
       "      <th>topic_prediction</th>\n",
       "      <th>prediction_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27401</th>\n",
       "      <td>The Blob starts with one of the most bizarre theme songs ever, sung by an uncredited Burt Bacharach of all people! You really have to hear it to believe it, The Blob may be worth watching just for this song alone &amp; my user comment summary is just a little taste of the classy lyrics... After this unnerving opening credits sequence The Blob introduces us, the viewer that is, to Steve Andrews (Steve McQueen as Steven McQueen) &amp; his girlfriend Jane Martin (Aneta Corsaut) who are parked on their own somewhere &amp; witness what looks like a meteorite falling to Earth in nearby woods. An old man (Olin Howland as Olin Howlin) who lives in a cabin also sees it &amp; goes to investigate, he finds a crater &amp; a strange football sized rock which splits open when he unwisely pokes it with a stick. Laying in the centre of the meteorite is a strange jelly like substance which sticks to the stick, if you know what I mean! It then slides up the stick &amp; attachés itself to the old man's hand. Meanwhile Steve &amp; Jane are quietly driving along minding their own business when the old man runs out in front of Steve's car, Steve being a decent kinda guy decides to take the old man to Dr. T. Hallan (Alden 'Stephen' Chase as Steven Chase) at the local surgery. Dr. Hallan says he doesn't know what the substance on the old man's hand is but it's getting bigger &amp; asks Steve to go back where he found him &amp; see if he can find out what happened. Steve agrees but doesn't come up with anything &amp; upon returning to Dr. Hallan's surgery he witnesses the blob devouring him. The town's police, Lieutenant Dave (Earl Rowe) &amp; the teenage hating Sergeant Jim Bert (John Benson) unsurprisingly don't believe a word of it &amp; end up suspecting Steve &amp; his mates Al (Anthony Franke), Tony (Robert Fields) &amp; someone called 'Mooch' Miller (James Bonnet) of playing an elaborate practical joke on the police department. However as the blob continues to eat it's way through the town Steve sets about finding proof of it's existence &amp; convincing the police about the threat it posses not just to their town but the entire world!&lt;br /&gt;&lt;br /&gt;Directed Irvin S. Yeaworth Jr. &amp; an uncredited Russell S. Doughton Jr. I was throughly disappointed by this, the original 1958 version of The Blob. The script by Kay Linaker as Kate Phillips &amp; Theodore Simonson is an absolute bore &amp; extremely dull not making the most of it's strongest aspects. The Blob focuses on the tiresome dramatics &amp; conflicts between the teenagers &amp; police, in fact the majority of The Blob is spent on Steve trying to convince the police of the blob's existence. For most of the film the blob itself almost seems inconsequential &amp; somewhat forgotten. It only has two or three scenes for the fist hour &amp; a bit until the less than exciting climax when the adults &amp; teenagers have to work together to defeat the blob &amp; have a new found appreciation of each other afterwards, yuck! Why couldn't the blob just eat the lot of 'em? No explanation is given for what the blob is or it's origins other than it came from space, how long did it take them to come up with that? The dialogue is clunky &amp; silly as well, as are people's actions &amp; decision making, I love the part when a nurse named Kate (Lee Paton as Lee Payton, did anyone use their real name in this thing?) is confronted by the blob, she throws some acid over it &amp; calmly proclaims \"Doctor, nothing will stop it!\", how does she know 'nothing' will stop it exactly? There's no blood or violence so don't worry about that, the special effects on the blob itself aren't too bad considering but it barely has any screen time &amp; moves very slowly, a bit like the film in general actually. The acting is terrible, McQueen is supposed to be a teenager when in reality he was 28 years old &amp; it shows, he looks old enough to be his own dad! Same thing goes for most of the other 'teenage' cast members &amp; everyone generally speaking are wooden &amp; unconvincing in their roles. Technically The Blob is very basic, dark static photography, dull direction &amp; forgettable production values. The Blob is one of those films that probably sounds good on paper &amp; is well known as being a 'classic' but is in actual fact a huge disappointment when finally seen. This is one case when the remake The Blob (1988) is definitely better than the original. The original Blob is slow &amp; boring &amp; the remake isn't, the original Blob contains no blood or gore &amp; the remake does, the original Blob has incredibly poor acting &amp; casting decisions &amp; the remake doesn't &amp; the original Blob itself gets very little screen time eating only three or four people throughout the entire film &amp; the remake features the blob all the way through &amp; it virtually eats an entire town. The choice is an easy one, the remake every time as it's a better film in every respect. I'll give the film two stars &amp; give that wonderful main theme song one on it's own. Definitely not the classic many seem to make out.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.979610</td>\n",
       "      <td>[0.020390067249536514, 0.9796099662780762]</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13566</th>\n",
       "      <td>This is the first recorded effort to put sound with a movie, and a the oldest that, obviously, is still in existence. This historic piece of film is the opening segment in the \"More Treasures Of The Natural Archives\" DVD.&lt;br /&gt;&lt;br /&gt;It's only a 15-second clip of a man playing a violin in front of a huge recording cylinder. Next to him are two men dancing. Near the end, another man walks on the stage. William Dickson, the director of this experiment, is the violin player. This \"movie\" had several titles over the years but the sound experiment was not really a success. It took over 30 years from this point to the synchronize sight and sound to the point where something could be issued to the public for entertainment. However, this was a start, no matter how primitive it came off. &lt;br /&gt;&lt;br /&gt;For more of the technical information and history of this film process, see the other review here by \"Boba Fett1138.\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>[0.0006713325274176896, 0.9993287324905396]</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>Some people like to tell you that Deep Space 9 is the best of all the Star Trek shows, because it stresses character development and continuity, and features a more complex background and ongoing plots. In some ways this makes it more satisfying, but in many ways the show fails entirely.&lt;br /&gt;&lt;br /&gt;The series starts out as a soap opera on a space station, with two entire seasons of generic science fiction stories balanced with banal subplots about the characters. The characters are a good bunch, and most of the actors are decent, but I think the writers tried too hard to make them \"normal\". By \"normal\" they actually mean \"ordinary and tedious\".&lt;br /&gt;&lt;br /&gt;At the end of Season Two we are introduced to the Dominion, who hang around menacingly for a while before finally going to war with the good guys in Season Five. This is the main \"story arc\" of the show, but it only takes up a fraction of the entire series. We still get lame stand-alone episodes, heroes still get stranded on weird planets for forty-five minutes, and there's an awful lot of low-brow comedy featuring the greedy, goofy Ferengi. A lot of episodes are merely dull, and some are unwatchable.&lt;br /&gt;&lt;br /&gt;The Dominion, DS9's main villains, are bent on galactic domination for the convenient reason that, well, they just don't like anyone. The entire war is presented with a naive lack of moral complexity and imagination. Impressively pyrotechnic space battles appear with great frequency from Season Three onwards, but these are carried out in ludicrously simplistic ways, such as two huge fleets of super-advanced starships flying right at each other and blasting away. The writers of DS9 (including the talented Ronald D. Moore, later of \"Battlestar Galactica\" fame) spiced up their monotonous show by starting a war, but at heart it is still a pedantic soap.&lt;br /&gt;&lt;br /&gt;DS9 remains a very frustrating experience. The continuous story is too flat and obvious to be really gripping, and the characters never truly develop in interesting ways. \"Babylon 5\" and \"Battlestar Galactica\" both fulfilled the promise made by DS9, and did everything much better. For Star Trek, stick with the original and the Next Generation.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>[6.19053389527835e-05, 0.9999381303787231]</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25188</th>\n",
       "      <td>VAMPYRES &lt;br /&gt;&lt;br /&gt;Aspect ratio: 1.85:1&lt;br /&gt;&lt;br /&gt;Sound format: Mono&lt;br /&gt;&lt;br /&gt;A motorist (Murray Brown) is lured to an isolated country house inhabited by two beautiful young women (Marianne Morris and Anulka) and becomes enmeshed in their free-spirited sexual lifestyle, but his hosts turn out to be vampires with a frenzied lust for human blood...&lt;br /&gt;&lt;br /&gt;Taking its cue from the lesbian vampire cycle initiated by maverick director Jean Rollin in France, and consolidated by the success of Hammer's \"Carmilla\" series in the UK, Jose Ramon Larraz' daring shocker VAMPYRES pushed the concept of Adult Horror much further than British censors were prepared to tolerate in 1974, and his film was cut by almost three minutes on its original British release. It isn't difficult to see why! Using its Gothic theme as the pretext for as much nudity, sex and bloodshed as the film's short running time will allow, Larraz (who wrote the screenplay under the pseudonym 'D. Daubeney') uses these commercial elements as mere backdrop to a languid meditation on life, death and the impulses - sexual and otherwise - which affirm the human condition.&lt;br /&gt;&lt;br /&gt;Shot on location at a picturesque country house during the Autumn of 1973, Harry Waxman's haunting cinematography conjures an atmosphere of grim foreboding, in which the desolate countryside - bleak and beautiful in equal measure - seems to foreshadow a whirlwind of impending horror (Larraz pulled a similar trick earlier the same year with SYMPTOMS, a low-key thriller which erupts into a frenzy of violence during the final reel). However, despite its pretensions, VAMPYRES' wafer-thin plot and rough-hewn production values will divide audiences from the outset, and while the two female protagonists are as charismatic and appealing as could be wished, the male lead (Brown, past his prime at the time of filming) is woefully miscast in a role that should have gone to some beautiful twentysomething stud. A must-see item for cult movie fans, an amusing curio for everyone else, VAMPYRES is an acquired taste. Watch out for silent era superstar Bessie Love in a brief cameo at the end of the movie.</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.995948</td>\n",
       "      <td>[0.0040516238659620285, 0.9959483742713928]</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                review  \\\n",
       "27401  The Blob starts with one of the most bizarre theme songs ever, sung by an uncredited Burt Bacharach of all people! You really have to hear it to believe it, The Blob may be worth watching just for this song alone & my user comment summary is just a little taste of the classy lyrics... After this unnerving opening credits sequence The Blob introduces us, the viewer that is, to Steve Andrews (Steve McQueen as Steven McQueen) & his girlfriend Jane Martin (Aneta Corsaut) who are parked on their own somewhere & witness what looks like a meteorite falling to Earth in nearby woods. An old man (Olin Howland as Olin Howlin) who lives in a cabin also sees it & goes to investigate, he finds a crater & a strange football sized rock which splits open when he unwisely pokes it with a stick. Laying in the centre of the meteorite is a strange jelly like substance which sticks to the stick, if you know what I mean! It then slides up the stick & attachés itself to the old man's hand. Meanwhile Steve & Jane are quietly driving along minding their own business when the old man runs out in front of Steve's car, Steve being a decent kinda guy decides to take the old man to Dr. T. Hallan (Alden 'Stephen' Chase as Steven Chase) at the local surgery. Dr. Hallan says he doesn't know what the substance on the old man's hand is but it's getting bigger & asks Steve to go back where he found him & see if he can find out what happened. Steve agrees but doesn't come up with anything & upon returning to Dr. Hallan's surgery he witnesses the blob devouring him. The town's police, Lieutenant Dave (Earl Rowe) & the teenage hating Sergeant Jim Bert (John Benson) unsurprisingly don't believe a word of it & end up suspecting Steve & his mates Al (Anthony Franke), Tony (Robert Fields) & someone called 'Mooch' Miller (James Bonnet) of playing an elaborate practical joke on the police department. However as the blob continues to eat it's way through the town Steve sets about finding proof of it's existence & convincing the police about the threat it posses not just to their town but the entire world!<br /><br />Directed Irvin S. Yeaworth Jr. & an uncredited Russell S. Doughton Jr. I was throughly disappointed by this, the original 1958 version of The Blob. The script by Kay Linaker as Kate Phillips & Theodore Simonson is an absolute bore & extremely dull not making the most of it's strongest aspects. The Blob focuses on the tiresome dramatics & conflicts between the teenagers & police, in fact the majority of The Blob is spent on Steve trying to convince the police of the blob's existence. For most of the film the blob itself almost seems inconsequential & somewhat forgotten. It only has two or three scenes for the fist hour & a bit until the less than exciting climax when the adults & teenagers have to work together to defeat the blob & have a new found appreciation of each other afterwards, yuck! Why couldn't the blob just eat the lot of 'em? No explanation is given for what the blob is or it's origins other than it came from space, how long did it take them to come up with that? The dialogue is clunky & silly as well, as are people's actions & decision making, I love the part when a nurse named Kate (Lee Paton as Lee Payton, did anyone use their real name in this thing?) is confronted by the blob, she throws some acid over it & calmly proclaims \"Doctor, nothing will stop it!\", how does she know 'nothing' will stop it exactly? There's no blood or violence so don't worry about that, the special effects on the blob itself aren't too bad considering but it barely has any screen time & moves very slowly, a bit like the film in general actually. The acting is terrible, McQueen is supposed to be a teenager when in reality he was 28 years old & it shows, he looks old enough to be his own dad! Same thing goes for most of the other 'teenage' cast members & everyone generally speaking are wooden & unconvincing in their roles. Technically The Blob is very basic, dark static photography, dull direction & forgettable production values. The Blob is one of those films that probably sounds good on paper & is well known as being a 'classic' but is in actual fact a huge disappointment when finally seen. This is one case when the remake The Blob (1988) is definitely better than the original. The original Blob is slow & boring & the remake isn't, the original Blob contains no blood or gore & the remake does, the original Blob has incredibly poor acting & casting decisions & the remake doesn't & the original Blob itself gets very little screen time eating only three or four people throughout the entire film & the remake features the blob all the way through & it virtually eats an entire town. The choice is an easy one, the remake every time as it's a better film in every respect. I'll give the film two stars & give that wonderful main theme song one on it's own. Definitely not the classic many seem to make out.   \n",
       "13566  This is the first recorded effort to put sound with a movie, and a the oldest that, obviously, is still in existence. This historic piece of film is the opening segment in the \"More Treasures Of The Natural Archives\" DVD.<br /><br />It's only a 15-second clip of a man playing a violin in front of a huge recording cylinder. Next to him are two men dancing. Near the end, another man walks on the stage. William Dickson, the director of this experiment, is the violin player. This \"movie\" had several titles over the years but the sound experiment was not really a success. It took over 30 years from this point to the synchronize sight and sound to the point where something could be issued to the public for entertainment. However, this was a start, no matter how primitive it came off. <br /><br />For more of the technical information and history of this film process, see the other review here by \"Boba Fett1138.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "17377  Some people like to tell you that Deep Space 9 is the best of all the Star Trek shows, because it stresses character development and continuity, and features a more complex background and ongoing plots. In some ways this makes it more satisfying, but in many ways the show fails entirely.<br /><br />The series starts out as a soap opera on a space station, with two entire seasons of generic science fiction stories balanced with banal subplots about the characters. The characters are a good bunch, and most of the actors are decent, but I think the writers tried too hard to make them \"normal\". By \"normal\" they actually mean \"ordinary and tedious\".<br /><br />At the end of Season Two we are introduced to the Dominion, who hang around menacingly for a while before finally going to war with the good guys in Season Five. This is the main \"story arc\" of the show, but it only takes up a fraction of the entire series. We still get lame stand-alone episodes, heroes still get stranded on weird planets for forty-five minutes, and there's an awful lot of low-brow comedy featuring the greedy, goofy Ferengi. A lot of episodes are merely dull, and some are unwatchable.<br /><br />The Dominion, DS9's main villains, are bent on galactic domination for the convenient reason that, well, they just don't like anyone. The entire war is presented with a naive lack of moral complexity and imagination. Impressively pyrotechnic space battles appear with great frequency from Season Three onwards, but these are carried out in ludicrously simplistic ways, such as two huge fleets of super-advanced starships flying right at each other and blasting away. The writers of DS9 (including the talented Ronald D. Moore, later of \"Battlestar Galactica\" fame) spiced up their monotonous show by starting a war, but at heart it is still a pedantic soap.<br /><br />DS9 remains a very frustrating experience. The continuous story is too flat and obvious to be really gripping, and the characters never truly develop in interesting ways. \"Babylon 5\" and \"Battlestar Galactica\" both fulfilled the promise made by DS9, and did everything much better. For Star Trek, stick with the original and the Next Generation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "25188  VAMPYRES <br /><br />Aspect ratio: 1.85:1<br /><br />Sound format: Mono<br /><br />A motorist (Murray Brown) is lured to an isolated country house inhabited by two beautiful young women (Marianne Morris and Anulka) and becomes enmeshed in their free-spirited sexual lifestyle, but his hosts turn out to be vampires with a frenzied lust for human blood...<br /><br />Taking its cue from the lesbian vampire cycle initiated by maverick director Jean Rollin in France, and consolidated by the success of Hammer's \"Carmilla\" series in the UK, Jose Ramon Larraz' daring shocker VAMPYRES pushed the concept of Adult Horror much further than British censors were prepared to tolerate in 1974, and his film was cut by almost three minutes on its original British release. It isn't difficult to see why! Using its Gothic theme as the pretext for as much nudity, sex and bloodshed as the film's short running time will allow, Larraz (who wrote the screenplay under the pseudonym 'D. Daubeney') uses these commercial elements as mere backdrop to a languid meditation on life, death and the impulses - sexual and otherwise - which affirm the human condition.<br /><br />Shot on location at a picturesque country house during the Autumn of 1973, Harry Waxman's haunting cinematography conjures an atmosphere of grim foreboding, in which the desolate countryside - bleak and beautiful in equal measure - seems to foreshadow a whirlwind of impending horror (Larraz pulled a similar trick earlier the same year with SYMPTOMS, a low-key thriller which erupts into a frenzy of violence during the final reel). However, despite its pretensions, VAMPYRES' wafer-thin plot and rough-hewn production values will divide audiences from the outset, and while the two female protagonists are as charismatic and appealing as could be wished, the male lead (Brown, past his prime at the time of filming) is woefully miscast in a role that should have gone to some beautiful twentysomething stud. A must-see item for cult movie fans, an amusing curio for everyone else, VAMPYRES is an acquired taste. Watch out for silent era superstar Bessie Love in a brief cameo at the end of the movie.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "\n",
       "      sentiment  max_prob                                 softmax_pred  \\\n",
       "27401  negative  0.979610  [0.020390067249536514, 0.9796099662780762]    \n",
       "13566  negative  0.999329  [0.0006713325274176896, 0.9993287324905396]   \n",
       "17377  negative  0.999938  [6.19053389527835e-05, 0.9999381303787231]    \n",
       "25188  negative  0.995948  [0.0040516238659620285, 0.9959483742713928]   \n",
       "\n",
       "       argmax topic_prediction prediction_correct  \n",
       "27401  1       positive         incorrect          \n",
       "13566  1       positive         incorrect          \n",
       "17377  1       positive         incorrect          \n",
       "25188  1       positive         incorrect          "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "test_df['prediction_correct'] = np.where(test_df['topic_prediction']==test_df['sentiment'], \"correct\", \"incorrect\")\n",
    "test_df.loc[test_df['sentiment'] != test_df['topic_prediction']].sort_values(['sentiment'])[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdcd1b6",
   "metadata": {},
   "source": [
    "# Predict on Individual Query (Get Ready for Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd9e8f",
   "metadata": {},
   "source": [
    "If we were to put this model into production we may want to individually predict the sentiment of each movie review as soon as they are submitted on the website. To do this we first need to load the trained model and prepare the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c0f48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e84eb2d47f418cb6fd35f4291638e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Name of the BERT model to use\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
    "\n",
    "max_length = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8816baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This was a brilliant movie, I really enjoyed it and would recommend it to anyone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d227a1",
   "metadata": {},
   "source": [
    "Now tokenize and pad the sentence we want to predict the sentiment of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d23ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this padding wont work as the longest in the batch is probably less than 100\n",
    "# therefore we pad separately below\n",
    "sentence_tokens = tokenizer(\n",
    "    text=sentence,\n",
    "    add_special_tokens=True,\n",
    "    padding=True, \n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "attention_mask_padded = pad_sequences(sentence_tokens['attention_mask'], maxlen=max_length, padding=\"post\")\n",
    "tokens_padded = pad_sequences(sentence_tokens['input_ids'], maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6a4f6",
   "metadata": {},
   "source": [
    "Finally get the model prediction and run it through the softmax function to get prediction probabilites and the predicted topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f560b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'topic': 'negative', 'confidence': 5.8824266e-06}, {'topic': 'positive', 'confidence': 0.99999416}]\n",
      "Predicted topic: ['positive']\n"
     ]
    }
   ],
   "source": [
    "text_prediction = model.predict([tokens_padded, attention_mask_padded])\n",
    "probabilities = tf.nn.softmax(text_prediction)\n",
    "probabilities_array = list(np.array(probabilities)[0])\n",
    "topic_list = list(topic_dict.values())\n",
    "\n",
    "return_array = []\n",
    "for topic, probability in list(zip(topic_list, probabilities_array)):\n",
    "    return_array.append(\n",
    "        {\"topic\": topic, \"confidence\": probability}\n",
    "    )\n",
    "print(return_array)\n",
    "print(f'Predicted topic: {np.vectorize(topic_dict.get)(text_prediction.argmax(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f3219",
   "metadata": {},
   "source": [
    "# Additional Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3a752",
   "metadata": {},
   "source": [
    "In some text classification tasks you may want to predict a certain amount of classes and everything else that does not fall into these classes can be considered \"other\". \"Other\" topic samples should be included in the data, however a model will not be trained for the \"other\" topic and these labels will be removed from the one hot encodings. This way any text that does not have a topic with a prediction probability greater than a predefined threshold (eg 0.8) can be classified as \"other\". The function below can be used to remove the \"other\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "620ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_others_encoding(one_hot_column, other_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to remove the \"other\" topic index from the one hot encoded labels\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    one_hot_column = one_hot_column[:other_index] + one_hot_column[other_index+1:]\n",
    "    return one_hot_column\n",
    "\n",
    "other_index = train_df['label'].cat.categories.to_list().index('other')\n",
    "train_df['one_hot_correct'] = train_df.apply(lambda x: self._remove_others_encoding(x['one_hot'], other_index),axis=1)\n",
    "val_df['one_hot_correct'] = val_df.apply(lambda x: self._remove_others_encoding(x['one_hot'], other_index),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050f341",
   "metadata": {},
   "source": [
    "If the training set is imbalanced the below code is one way it can be balanced, this again assumes there is an \"other\" class in the data and this is the largest class. If this is not the case it can be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffe728",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(train_df['label'].drop_duplicates())\n",
    "topics.remove(\"other\")\n",
    "\n",
    "# balance the train dataset\n",
    "df = pd.DataFrame()\n",
    "for topic in topics:\n",
    "    resampled_data = resample(train_df.loc[train_df['label']==f\"{topic}\"],\n",
    "                             replace=True,     # sample with replacement\n",
    "#                                  n_samples=14000,\n",
    "                             n_samples=len(train_df.loc[train_df['label']=='other']),\n",
    "                             random_state=11) # reproducible results\n",
    "    df = df.append(resampled_data.loc[resampled_data['label']==f\"{topic}\"])\n",
    "df = df.append(train_df.loc[train_df['label']==\"other\"])\n",
    "train_df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c1377",
   "metadata": {},
   "source": [
    "# Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc6c72",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a\n",
    "\n",
    "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
